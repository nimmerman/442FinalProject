{\rtf1\ansi\ansicpg1252\cocoartf1265
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10320\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs26 \cf0 Before running the algorithm described in the paper, we need to collect various pieces of information about the image. We have written the code to collect occluding boundaries of objects in the image, to partition these boundaries in to patches, to find the normal vectors of these patches and to extrapolate the intensity of the occluding pixels at these patches as well as began creating the GUI that will stitch all of the pieces together. In accordance with the method described in the paper, we determined the best way to collect the occluding boundaries of objects is through user input. We began the making the GUI by displaying the potentially forged image and allowing the user to continuously add occluding boundaries to the system. After the boundaries are collected it is partitioned into patches (implemented). To estimate the normal vectors for each patch we are going need to collect three user inputed points for each patch (not implemented yet) and then use those three points to fit a quadratic curve to estimate the surface of the patch which can then be used to estimate the normal vector (implemented). We can then use the normals to estimate the intensity of the pixels at the occluding boundaries (implemented). At this point we have coded but not tested the mechanisms to collect almost all of the information that we need to be able to run the algorithm described in the paper.}